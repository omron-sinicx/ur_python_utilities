from tabnanny import verbose
import numpy as np
import rospy
import gym
from gym.utils import seeding
from ur3e_openai.peg_gazebo_connection import GazeboConnection, RobotConnection
from ur3e_openai.controllers_connection import ControllersConnection
from std_msgs.msg import Bool
import ur_control.log as utils
color_log = utils.TextColors()

# https://github.com/openai/gym/blob/master/gym/core.py


class RobotGazeboEnv(gym.Env):
    def __init__(self,
                 robot_name_space,
                 reset_controls,
                 use_gazebo=False,
                 seed=None,
                 **kwargs):

        # To reset Simulations
        rospy.loginfo("START init RobotGazeboEnv")
        if use_gazebo:
            self.robot_connection = GazeboConnection(**kwargs)
        else:
            self.robot_connection = RobotConnection()
            self.subs_pause = rospy.Subscriber('ur3e_openai/pause', Bool, self.pause_callback, queue_size=1)

        self.controllers_object = ControllersConnection(namespace=robot_name_space)
        self.reset_controls = reset_controls
        self.seed(seed)

        # Set up ROS related variables
        self.episode_num = 0
        self.episode_hist = np.array([0,1]*10)
        self.success_end = False # whether the episode ends because of success
        self.step_count = 0
        self.total_steps = 0
        self.cumulated_episode_reward = 0
        self.pause = False
        self._log_message = None
        rospy.logdebug("END init RobotGazeboEnv")

    def pause_callback(self, msg):
        if msg.data is True:
            self.pause = True

    # Env methods
    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        """
        Function executed each time step.
        Here we get the action execute it in a time step and retrieve the
        observations generated by that action.
        :param action:
        :return: obs, reward, done, info
        """
        """
        Here we should convert the action num to movement action, execute the action in the
        simulation and get the observations result of performing that action.
        """
        if self.pause:
            self._pause_env()
            self.pause = False

        # self.robot_connection.unpause()
        self._set_action(action)
        # self.robot_connection.pause()
        obs = self._get_obs()
        # incrementing the counters
        self.step_count += 1
        self.total_steps += 1
        # state variables
        done = self._is_done(obs)
        info = self._get_info(obs)
        reward, reward_details = self._compute_reward(obs, done)
        self.cumulated_episode_reward += reward
        self.cumulated_reward_details += reward_details
        
        # if done:
        #     self.logger.info("reward details: %s, total: %s" % (np.round(self.cumulated_reward_details, 2), round(self.cumulated_episode_reward, 2)))

        return obs, reward, done, info

    def reset(self):
        rospy.logdebug("Reseting RobotGazeboEnvironment")
        self._update_episode()
        self._init_env_variables()
        self.step_count = 0
        self.cumulated_dist = 0
        self.cumulated_force = 0
        self.cumulated_jerk = 0
        self.cumulated_vel = 0
        self.cumulated_reward_details = np.zeros(7)
        self._reset_sim()
        obs = self._get_obs()
        rospy.logdebug("END Reseting RobotGazeboEnvironment")
        return obs

    def close(self):
        """
        Function executed when closing the environment.
        Use it for closing GUIS and other systems that need closing.
        :return:
        """
        rospy.logdebug("Closing RobotGazeboEnvironment")
        rospy.signal_shutdown("Closing RobotGazeboEnvironment")

    def _update_episode(self):
        """
        Publishes the cumulated reward of the episode and
        increases the episode number by one.
        :return:
        """
        if self._log_message is not None:
            color_log.ok("\n>> End of Episode = %s, Reward= %s, steps=%s" %
                         (self.episode_num, self.cumulated_episode_reward, self.step_count))
            color_log.warning(self._log_message)

        self.episode_hist[0:-1] = self.episode_hist[1:]
        self.episode_hist[-1] = 1.0 if self.success_end else 0.0
        self.episode_num += 1
        self.cumulated_episode_reward = 0
        self.success_end = False

    # Extension methods
    # ----------------------------

    def _reset_sim(self):
        """Resets a simulation
        """
        rospy.logdebug("RESET SIM START")
        if self.reset_controls:
            rospy.logdebug("RESET CONTROLLERS")
            self.robot_connection.unpause()
            self.controllers_object.reset_controllers()
            self._check_all_systems_ready()
            self._set_init_pose()
            self.robot_connection.pause()
            self.robot_connection.reset()
            self.robot_connection.unpause()
            self.controllers_object.reset_controllers()
            # self._check_all_systems_ready()
            self.robot_connection.pause()

        else:
            rospy.logdebug("DONT RESET CONTROLLERS")
            self.robot_connection.unpause()
            self._check_all_systems_ready()
            self._set_init_pose()
            self.robot_connection.pause()
            self.robot_connection.reset()
            self.robot_connection.unpause()

        rospy.logdebug("RESET SIM END")
        return True

    def _set_init_pose(self):
        """Sets the Robot in its init pose
        """
        raise NotImplementedError()

    def _check_all_systems_ready(self):
        """
        Checks that all the sensors, publishers and other simulation systems are
        operational.
        """
        raise NotImplementedError()

    def _get_obs(self):
        """Returns the observation.
        """
        raise NotImplementedError()

    def _get_info(self, obs):
        """Returns additional information (optional)
        """
        return {}

    def _init_env_variables(self):
        """Inits variables needed to be initialised each time we reset at the start
        of an episode.
        """
        raise NotImplementedError()

    def _set_action(self, action):
        """Applies the given action to the simulation.
        """
        raise NotImplementedError()

    def _is_done(self, observations):
        """Indicates whether or not the episode is done ( the robot has fallen for example).
        """
        raise NotImplementedError()

    def _compute_reward(self, observations, done):
        """Calculates the reward to give based on the observations given.
        """
        raise NotImplementedError()

    def _env_setup(self, initial_qpos):
        """Initial configuration of the environment. Can be used to configure initial state
        and extract information from the simulation.
        """
        raise NotImplementedError()

    def _pause_env(self):
        """Perform any validation/checks before/after pausing environment"""
        raise NotImplementedError()
